{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-hoc Repacking Analysis for H2O Molecule\n",
    "\n",
    "This notebook demonstrates three Pauli grouping strategies:\n",
    "1. **Baseline**: Sorted insertion grouping\n",
    "2. **Greedy Repacking**: Optimize grouping from scratch for minimum variance\n",
    "3. **Post-hoc Repacking**: Add paulis from previous groups if they're diagonal under existing circuits\n",
    "\n",
    "We compare these methods on both simulated and real IBM hardware data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\nimport openfermion as of\nimport cirq\nfrom collections import Counter\n\nfrom src.grouping import MeasurementGroups, sorted_insertion_grouping\nfrom src.repacking import greedy_repacking, posthoc_repacking\nfrom src.measurement import create_measurement_setups, create_posthoc_setups\nfrom src.measurement.estimation import estimate_from_groups\nfrom src.grouping import GroupingCache"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load H2O Hamiltonian\n",
    "\n",
    "Load the water molecule Hamiltonian (14 qubits, 1620 Pauli terms) and compute exact HF energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load molecular data\nmol_data = of.chem.MolecularData(filename='../data/hamiltonians/monomer_eqb.hdf5')\nn_electrons = mol_data.n_electrons\n\n# Convert to qubit Hamiltonian\nhamiltonian_fermion = of.jordan_wigner(\n    of.get_fermion_operator(mol_data.get_molecular_hamiltonian())\n)\nhamiltonian = of.qubit_operator_to_pauli_sum(hamiltonian_fermion)\n\n# Setup qubits\nqubits = cirq.LineQubit.range(14)\n\n# Compute exact HF energy\nhf_energy_exact = 0.0\nfor pauli_string in hamiltonian:\n    coeff = pauli_string.coefficient\n    exp_value = 1.0\n    for qubit in qubits:\n        if qubit in pauli_string.qubits:\n            gate = pauli_string[qubit]\n            qubit_idx = qubit.x\n            if qubit_idx < n_electrons:\n                if gate == cirq.Z:\n                    exp_value *= -1\n                elif gate in [cirq.X, cirq.Y]:\n                    exp_value = 0.0\n                    break\n            else:\n                if gate in [cirq.X, cirq.Y]:\n                    exp_value = 0.0\n                    break\n    hf_energy_exact += np.real(coeff * exp_value)\n\nprint(f\"Hamiltonian: {len(hamiltonian)} Pauli terms on {len(qubits)} qubits\")\nprint(f\"Exact HF energy: {hf_energy_exact:.6f} Ha\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Sorted Insertion Grouping\n",
    "\n",
    "Group Pauli terms using sorted insertion - the standard greedy algorithm that groups commuting terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Grouping Statistics:\n",
      "  Number of groups: 65\n",
      "  Total paulis: 1619\n",
      "  Average paulis per group: 24.9\n",
      "  Min paulis in a group: 2\n",
      "  Max paulis in a group: 105\n"
     ]
    }
   ],
   "source": [
    "# Perform sorted insertion grouping\n",
    "baseline_groups = sorted_insertion_grouping(hamiltonian)\n",
    "baseline_setups = create_measurement_setups(baseline_groups.groups, qubits)\n",
    "\n",
    "# Statistics\n",
    "num_groups_baseline = len(baseline_groups.groups)\n",
    "num_paulis_baseline = sum(len(g) for g in baseline_groups.groups)\n",
    "avg_paulis_per_group_baseline = num_paulis_baseline / num_groups_baseline\n",
    "\n",
    "print(f\"Baseline Grouping Statistics:\")\n",
    "print(f\"  Number of groups: {num_groups_baseline}\")\n",
    "print(f\"  Total paulis: {num_paulis_baseline}\")\n",
    "print(f\"  Average paulis per group: {avg_paulis_per_group_baseline:.1f}\")\n",
    "print(f\"  Min paulis in a group: {min(len(g) for g in baseline_groups.groups)}\")\n",
    "print(f\"  Max paulis in a group: {max(len(g) for g in baseline_groups.groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Repacking\n",
    "\n",
    "Repack strings to minimize expected variance.  This does a second greedy pass scoring strings according to c_i^2/N_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Repacking Statistics:\n",
      "  Number of groups: 65\n",
      "  Total paulis: 5057\n",
      "  Average paulis per group: 77.8\n",
      "  Min paulis in a group: 33\n",
      "  Max paulis in a group: 105\n",
      "\n",
      "Comparison to Baseline:\n",
      "  Paulis added: 3438 (212% increase)\n",
      "  Groups changed: 65 (same)\n"
     ]
    }
   ],
   "source": [
    "# Perform greedy repacking\n",
    "greedy_groups = greedy_repacking(baseline_groups, verbose=False)\n",
    "greedy_setups = create_measurement_setups(greedy_groups.groups, qubits)\n",
    "\n",
    "# Statistics\n",
    "num_groups_greedy = len(greedy_groups.groups)\n",
    "num_paulis_greedy = sum(len(g) for g in greedy_groups.groups)\n",
    "avg_paulis_per_group_greedy = num_paulis_greedy / num_groups_greedy\n",
    "\n",
    "print(f\"Greedy Repacking Statistics:\")\n",
    "print(f\"  Number of groups: {num_groups_greedy}\")\n",
    "print(f\"  Total paulis: {num_paulis_greedy}\")\n",
    "print(f\"  Average paulis per group: {avg_paulis_per_group_greedy:.1f}\")\n",
    "print(f\"  Min paulis in a group: {min(len(g) for g in greedy_groups.groups)}\")\n",
    "print(f\"  Max paulis in a group: {max(len(g) for g in greedy_groups.groups)}\")\n",
    "\n",
    "# Comparison to baseline\n",
    "paulis_added_greedy = num_paulis_greedy - num_paulis_baseline\n",
    "percent_increase_greedy = (paulis_added_greedy / num_paulis_baseline) * 100\n",
    "\n",
    "print(f\"\\nComparison to Baseline:\")\n",
    "print(f\"  Paulis added: {paulis_added_greedy} ({percent_increase_greedy:.0f}% increase)\")\n",
    "print(f\"  Groups changed: {num_groups_baseline} (same)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc Repacking\n",
    "\n",
    "Add paulis from previous groups if they commute with current group AND are diagonal under the group's circuit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Perform post-hoc repacking\nposthoc_groups = posthoc_repacking(\n    baseline_groups,\n    baseline_setups,\n    qubits,\n    verbose=False\n)\n\n# CORRECTED: Use baseline circuits for post-hoc\n# This ensures we're extracting additional info from the SAME measurements\nposthoc_setups = create_posthoc_setups(\n    baseline_groups,\n    baseline_setups,\n    posthoc_groups,\n    qubits\n)\n\n# Statistics\nnum_groups_posthoc = len(posthoc_groups.groups)\nnum_paulis_posthoc = sum(len(g) for g in posthoc_groups.groups)\navg_paulis_per_group_posthoc = num_paulis_posthoc / num_groups_posthoc\n\nprint(f\"Post-hoc Repacking Statistics:\")\nprint(f\"  Number of groups: {num_groups_posthoc}\")\nprint(f\"  Total paulis: {num_paulis_posthoc}\")\nprint(f\"  Average paulis per group: {avg_paulis_per_group_posthoc:.1f}\")\nprint(f\"  Min paulis in a group: {min(len(g) for g in posthoc_groups.groups)}\")\nprint(f\"  Max paulis in a group: {max(len(g) for g in posthoc_groups.groups)}\")\n\n# Comparison to baseline\npaulis_added_posthoc = num_paulis_posthoc - num_paulis_baseline\npercent_increase_posthoc = (paulis_added_posthoc / num_paulis_baseline) * 100\n\nprint(f\"\\nComparison to Baseline:\")\nprint(f\"  Paulis added: {paulis_added_posthoc} ({percent_increase_posthoc:.0f}% increase)\")\nprint(f\"  Groups changed: {num_groups_baseline} (same)\")\n\n# Circuit reuse analysis\ncircuits_match = 0\ncircuits_differ = 0\n\nfor i in range(len(baseline_setups)):\n    baseline_circuit = baseline_setups[i].basis_rotation\n    posthoc_circuit = posthoc_setups[i].basis_rotation\n    \n    if baseline_circuit == posthoc_circuit:\n        circuits_match += 1\n    else:\n        circuits_differ += 1\n\nprint(f\"\\nCircuit Reuse:\")\nprint(f\"  Circuits matching baseline: {circuits_match}/{len(baseline_setups)} ({circuits_match/len(baseline_setups)*100:.0f}%)\")\nprint(f\"  (All should match for correct post-hoc analysis)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Results\n",
    "\n",
    "Run simulations with the Hartree-Fock state to compare variance reduction across all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulations with 100,000 shots per group...\n",
      "\n",
      "Simulating baseline...\n",
      "Simulating greedy repacking...\n",
      "Simulating post-hoc repacking...\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create HF state\n",
    "hf_state = [1] * n_electrons + [0] * (len(qubits) - n_electrons)\n",
    "shots_per_group = 100000\n",
    "\n",
    "print(f\"Running simulations with {shots_per_group:,} shots per group...\\n\")\n",
    "\n",
    "# Generate measurement counts for each method\n",
    "def simulate_measurements(groups, setups, state, shots):\n",
    "    \"\"\"Simulate measurements for all groups.\"\"\"\n",
    "    all_counts = []\n",
    "    for group_idx, (group, setup) in enumerate(zip(groups.groups, setups)):\n",
    "        circuit = setup.basis_rotation\n",
    "        \n",
    "        # Apply circuit to state and get probabilities\n",
    "        simulator = cirq.Simulator()\n",
    "        initial_state_cirq = cirq.Circuit([cirq.X(qubits[i]) for i, bit in enumerate(state) if bit == 1])\n",
    "        full_circuit = initial_state_cirq + circuit\n",
    "        result = simulator.simulate(full_circuit)\n",
    "        \n",
    "        # Sample from the final state\n",
    "        sampler = cirq.Simulator()\n",
    "        samples = sampler.run(full_circuit + cirq.measure(*qubits), repetitions=shots)\n",
    "        \n",
    "        # Convert to counts dictionary\n",
    "        counts = Counter()\n",
    "        for measurement in samples.measurements.values():\n",
    "            for bitstring in measurement:\n",
    "                key = ''.join(str(int(b)) for b in bitstring)\n",
    "                counts[key] += 1\n",
    "        \n",
    "        all_counts.append(counts)\n",
    "    \n",
    "    return all_counts\n",
    "\n",
    "# Simulate all three methods\n",
    "print(\"Simulating baseline...\")\n",
    "baseline_counts = simulate_measurements(baseline_groups, baseline_setups, hf_state, shots_per_group)\n",
    "\n",
    "print(\"Simulating greedy repacking...\")\n",
    "greedy_counts = simulate_measurements(greedy_groups, greedy_setups, hf_state, shots_per_group)\n",
    "\n",
    "print(\"Simulating post-hoc repacking...\")\n",
    "posthoc_counts = simulate_measurements(posthoc_groups, posthoc_setups, hf_state, shots_per_group)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Simulation Results (HF State, 100k shots/group)\n",
      "================================================================================\n",
      "\n",
      "Exact HF energy: -75.679017 Ha\n",
      "\n",
      "Baseline:\n",
      "  Energy: -75.680290 ± 0.004028 Ha\n",
      "  Error: 0.001273 Ha\n",
      "  Variance: 1.622725e-05\n",
      "\n",
      "Greedy Repacking:\n",
      "  Energy: -75.679137 ± 0.001768 Ha\n",
      "  Error: 0.000120 Ha\n",
      "  Variance: 3.126856e-06\n",
      "  Variance reduction: 5.19x\n",
      "\n",
      "Post-hoc Repacking:\n",
      "  Energy: -75.680305 ± 0.001941 Ha\n",
      "  Error: 0.001288 Ha\n",
      "  Variance: 3.768386e-06\n",
      "  Variance reduction: 4.31x\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Estimate energies from simulated counts\n",
    "shots_list = [shots_per_group] * num_groups_baseline\n",
    "\n",
    "baseline_sim_results = estimate_from_groups(\n",
    "    baseline_groups,\n",
    "    baseline_setups,\n",
    "    baseline_counts,\n",
    "    shots_list,\n",
    "    qubits\n",
    ")\n",
    "\n",
    "greedy_sim_results = estimate_from_groups(\n",
    "    greedy_groups,\n",
    "    greedy_setups,\n",
    "    greedy_counts,\n",
    "    shots_list,\n",
    "    qubits\n",
    ")\n",
    "\n",
    "posthoc_sim_results = estimate_from_groups(\n",
    "    posthoc_groups,\n",
    "    posthoc_setups,\n",
    "    posthoc_counts,\n",
    "    shots_list,\n",
    "    qubits\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"Simulation Results (HF State, 100k shots/group)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nExact HF energy: {hf_energy_exact:.6f} Ha\\n\")\n",
    "\n",
    "print(f\"Baseline:\")\n",
    "print(f\"  Energy: {baseline_sim_results.energy:.6f} ± {baseline_sim_results.energy_std():.6f} Ha\")\n",
    "print(f\"  Error: {abs(baseline_sim_results.energy - hf_energy_exact):.6f} Ha\")\n",
    "print(f\"  Variance: {baseline_sim_results.energy_variance:.6e}\")\n",
    "\n",
    "print(f\"\\nGreedy Repacking:\")\n",
    "print(f\"  Energy: {greedy_sim_results.energy:.6f} ± {greedy_sim_results.energy_std():.6f} Ha\")\n",
    "print(f\"  Error: {abs(greedy_sim_results.energy - hf_energy_exact):.6f} Ha\")\n",
    "print(f\"  Variance: {greedy_sim_results.energy_variance:.6e}\")\n",
    "print(f\"  Variance reduction: {baseline_sim_results.energy_variance / greedy_sim_results.energy_variance:.2f}x\")\n",
    "\n",
    "print(f\"\\nPost-hoc Repacking:\")\n",
    "print(f\"  Energy: {posthoc_sim_results.energy:.6f} ± {posthoc_sim_results.energy_std():.6f} Ha\")\n",
    "print(f\"  Error: {abs(posthoc_sim_results.energy - hf_energy_exact):.6f} Ha\")\n",
    "print(f\"  Variance: {posthoc_sim_results.energy_variance:.6e}\")\n",
    "print(f\"  Variance reduction: {baseline_sim_results.energy_variance / posthoc_sim_results.energy_variance:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load September 25 hardware data\nwith open('../data/hardware_results/all_counts_fez_sep25.pkl', 'rb') as f:\n    hardware_counts_sep = pickle.load(f)\n\nshots_per_group_hw = [sum(counts.values()) for counts in hardware_counts_sep]\ntotal_shots = sum(shots_per_group_hw)\n\nprint(f\"September 2025 Hardware Data:\")\nprint(f\"  Groups: {len(hardware_counts_sep)}\")\nprint(f\"  Total shots: {total_shots:,}\")\nprint(f\"  Shots per group: {shots_per_group_hw[0]:,}\\n\")\n\n# Analyze with baseline and post-hoc\nbaseline_hw_sep = estimate_from_groups(\n    baseline_groups,\n    baseline_setups,\n    hardware_counts_sep,\n    shots_per_group_hw,\n    qubits\n)\n\nposthoc_hw_sep = estimate_from_groups(\n    posthoc_groups,\n    posthoc_setups,\n    hardware_counts_sep,\n    shots_per_group_hw,\n    qubits\n)\n\nvariance_reduction_sep = baseline_hw_sep.energy_variance / posthoc_hw_sep.energy_variance\nstd_reduction_sep = baseline_hw_sep.energy_std() / posthoc_hw_sep.energy_std()\n\nprint(\"=\"*80)\nprint(\"September 25 Hardware Results\")\nprint(\"=\"*80)\nprint(f\"\\nExact HF energy: {hf_energy_exact:.6f} Ha\\n\")\n\nprint(f\"Baseline:\")\nprint(f\"  Energy: {baseline_hw_sep.energy:.6f} ± {baseline_hw_sep.energy_std():.6f} Ha\")\nprint(f\"  Error from exact: {abs(baseline_hw_sep.energy - hf_energy_exact):.6f} Ha\")\nprint(f\"  Variance: {baseline_hw_sep.energy_variance:.6e}\")\n\nprint(f\"\\nPost-hoc Repacking:\")\nprint(f\"  Energy: {posthoc_hw_sep.energy:.6f} ± {posthoc_hw_sep.energy_std():.6f} Ha\")\nprint(f\"  Error from exact: {abs(posthoc_hw_sep.energy - hf_energy_exact):.6f} Ha\")\nprint(f\"  Variance: {posthoc_hw_sep.energy_variance:.6e}\")\n\nprint(f\"\\nVariance reduction: {variance_reduction_sep:.2f}x ({(1-1/variance_reduction_sep)*100:.1f}% reduction)\")\nprint(f\"Std dev reduction: {std_reduction_sep:.2f}x\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load October 22 hardware data\nwith open('../data/hardware_results/all_counts_fez_oct22_3.pkl', 'rb') as f:\n    hardware_counts_oct = pickle.load(f)\n\nprint(f\"October 22 Hardware Data:\")\nprint(f\"  Groups: {len(hardware_counts_oct)}\")\nprint(f\"  Total shots: {sum(sum(counts.values()) for counts in hardware_counts_oct):,}\")\nprint(f\"  Shots per group: {sum(hardware_counts_oct[0].values()):,}\\n\")\n\n# Analyze with baseline and post-hoc\nbaseline_hw_oct = estimate_from_groups(\n    baseline_groups,\n    baseline_setups,\n    hardware_counts_oct,\n    shots_per_group_hw,\n    qubits\n)\n\nposthoc_hw_oct = estimate_from_groups(\n    posthoc_groups,\n    posthoc_setups,\n    hardware_counts_oct,\n    shots_per_group_hw,\n    qubits\n)\n\nvariance_reduction_oct = baseline_hw_oct.energy_variance / posthoc_hw_oct.energy_variance\nstd_reduction_oct = baseline_hw_oct.energy_std() / posthoc_hw_oct.energy_std()\n\nprint(\"=\"*80)\nprint(\"October 22 Hardware Results\")\nprint(\"=\"*80)\nprint(f\"\\nExact HF energy: {hf_energy_exact:.6f} Ha\\n\")\n\nprint(f\"Baseline:\")\nprint(f\"  Energy: {baseline_hw_oct.energy:.6f} ± {baseline_hw_oct.energy_std():.6f} Ha\")\nprint(f\"  Error from exact: {abs(baseline_hw_oct.energy - hf_energy_exact):.6f} Ha\")\nprint(f\"  Variance: {baseline_hw_oct.energy_variance:.6e}\")\n\nprint(f\"\\nPost-hoc Repacking:\")\nprint(f\"  Energy: {posthoc_hw_oct.energy:.6f} ± {posthoc_hw_oct.energy_std():.6f} Ha\")\nprint(f\"  Error from exact: {abs(posthoc_hw_oct.energy - hf_energy_exact):.6f} Ha\")\nprint(f\"  Variance: {posthoc_hw_oct.energy_variance:.6e}\")\n\nprint(f\"\\nVariance reduction: {variance_reduction_oct:.2f}x ({(1-1/variance_reduction_oct)*100:.1f}% reduction)\")\nprint(f\"Std dev reduction: {std_reduction_oct:.2f}x\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison Table\n",
    "\n",
    "Side-by-side comparison of all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "SUMMARY: All Results\n",
      "====================================================================================================\n",
      "Exact HF Energy: -75.679017 Ha\n",
      "====================================================================================================\n",
      "    Dataset   Method Energy (Ha) Std Dev (Ha) Variance Var. Reduction Circuit Reuse\n",
      " Simulation Baseline  -75.680290     0.004028 1.62e-05          1.00x          100%\n",
      " Simulation   Greedy  -75.679137     0.001768 3.13e-06          5.19x           N/A\n",
      " Simulation Post-hoc  -75.680305     0.001941 3.77e-06          4.31x          100%\n",
      "HW Sep 2025 Baseline  -73.551561     0.006340 4.02e-05          1.00x          100%\n",
      "HW Sep 2025 Post-hoc  -67.995741     0.011542 1.33e-04          0.30x          100%\n",
      "HW Oct 2022 Baseline  -52.837890     0.062171 3.87e-03          1.00x          100%\n",
      "HW Oct 2022 Post-hoc  -49.897433     0.016396 2.69e-04         14.38x          100%\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_data = [\n",
    "    {\n",
    "        'Dataset': 'Simulation',\n",
    "        'Method': 'Baseline',\n",
    "        'Energy (Ha)': f\"{baseline_sim_results.energy:.6f}\",\n",
    "        'Std Dev (Ha)': f\"{baseline_sim_results.energy_std():.6f}\",\n",
    "        'Variance': f\"{baseline_sim_results.energy_variance:.2e}\",\n",
    "        'Var. Reduction': '1.00x',\n",
    "        'Circuit Reuse': '100%'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'Simulation',\n",
    "        'Method': 'Greedy',\n",
    "        'Energy (Ha)': f\"{greedy_sim_results.energy:.6f}\",\n",
    "        'Std Dev (Ha)': f\"{greedy_sim_results.energy_std():.6f}\",\n",
    "        'Variance': f\"{greedy_sim_results.energy_variance:.2e}\",\n",
    "        'Var. Reduction': f\"{baseline_sim_results.energy_variance / greedy_sim_results.energy_variance:.2f}x\",\n",
    "        'Circuit Reuse': 'N/A'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'Simulation',\n",
    "        'Method': 'Post-hoc',\n",
    "        'Energy (Ha)': f\"{posthoc_sim_results.energy:.6f}\",\n",
    "        'Std Dev (Ha)': f\"{posthoc_sim_results.energy_std():.6f}\",\n",
    "        'Variance': f\"{posthoc_sim_results.energy_variance:.2e}\",\n",
    "        'Var. Reduction': f\"{baseline_sim_results.energy_variance / posthoc_sim_results.energy_variance:.2f}x\",\n",
    "        'Circuit Reuse': '100%'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'HW Sep 2025',\n",
    "        'Method': 'Baseline',\n",
    "        'Energy (Ha)': f\"{baseline_hw_sep.energy:.6f}\",\n",
    "        'Std Dev (Ha)': f\"{baseline_hw_sep.energy_std():.6f}\",\n",
    "        'Variance': f\"{baseline_hw_sep.energy_variance:.2e}\",\n",
    "        'Var. Reduction': '1.00x',\n",
    "        'Circuit Reuse': '100%'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'HW Sep 2025',\n",
    "        'Method': 'Post-hoc',\n",
    "        'Energy (Ha)': f\"{posthoc_hw_sep.energy:.6f}\",\n",
    "        'Std Dev (Ha)': f\"{posthoc_hw_sep.energy_std():.6f}\",\n",
    "        'Variance': f\"{posthoc_hw_sep.energy_variance:.2e}\",\n",
    "        'Var. Reduction': f\"{variance_reduction_sep:.2f}x\",\n",
    "        'Circuit Reuse': '100%'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'HW Oct 2022',\n",
    "        'Method': 'Baseline',\n",
    "        'Energy (Ha)': f\"{baseline_hw_oct.energy:.6f}\",\n",
    "        'Std Dev (Ha)': f\"{baseline_hw_oct.energy_std():.6f}\",\n",
    "        'Variance': f\"{baseline_hw_oct.energy_variance:.2e}\",\n",
    "        'Var. Reduction': '1.00x',\n",
    "        'Circuit Reuse': '100%'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'HW Oct 2022',\n",
    "        'Method': 'Post-hoc',\n",
    "        'Energy (Ha)': f\"{posthoc_hw_oct.energy:.6f}\",\n",
    "        'Std Dev (Ha)': f\"{posthoc_hw_oct.energy_std():.6f}\",\n",
    "        'Variance': f\"{posthoc_hw_oct.energy_variance:.2e}\",\n",
    "        'Var. Reduction': f\"{variance_reduction_oct:.2f}x\",\n",
    "        'Circuit Reuse': '100%'\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY: All Results\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Exact HF Energy: {hf_energy_exact:.6f} Ha\")\n",
    "print(\"=\"*100)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}